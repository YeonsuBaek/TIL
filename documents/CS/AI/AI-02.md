# 인공지능 - 지능적 에이전트

> ### Index
>
> 1. 에이전트와 환경
> 2. 좋은 행동: 합리성 개념
> 3. 환경의 본성
> 4. 에이전트의 구조
> 5. 요약

> ### 학습 목표
>
> 에이전트의 본성, 환경의 다양성, 그리고 에이전트의 다양한 종류를 논의한다.

## 1. 에이전트와 환경

에이전트와 환경의 상호작용 요소에는 감지기(sensor), 환경(environment), 작동기(actuator)가 있다.

![](https://velog.velcdn.com/images/yeonsubaek/post/8ebe4bdd-ab09-4368-a66b-9a481c4ebfb7/image.jpeg)

환경에서 지각(먼지나 장애물 여부)이 감지기에 들어오면 에이전트가 함수가 실행되어 상태를 추론하고 동작한다. 이때 정보와 환경에 맞는 올바른 행동을 해야 한다. 작동기를 통해 다시 환경으로 돌아가 동작이 수행된다.

![](https://velog.velcdn.com/images/yeonsubaek/post/753f755e-2300-47cb-9f8c-226be2119279/image.jpeg)

사각형 A와 B라는 장소로 이루어진 진공청소기 세계가 있다고 가정하자.
진공청소기 에이전트는 자신이 어디에 있는지, 그 공간이 깨끗한지 더러운지 환경에 대한 정보를 지각한다.  
에이전트가 선택할 수 있는 동작은 왼쪽이나 오른쪽으로 이동하거나 먼지를 흡입하거나 아무 일도 하지 않는 것이다.

위와 같은 인식 순서에 대한 동작을 표로 나타내면 다음과 같다.

| 인식 순서              | 동작  |
| ---------------------- | ----- |
| [A, Clean]             | Right |
| [A, Dirty]             | Suck  |
| [B, Clean]             | Left  |
| [B, Dirty]             | Suck  |
| [A, Clean], [A, Clean] | Right |
| [A, Clean], [A, Dirty] | Suck  |
| ...                    | ...   |

인식 순서의 경우의 수가 너무 많다. 어떻게 하면 효율적으로 처리할 수 있을까?

## 2. 좋은 행동: 합리성 개념

### 2-1. 성과 측도

합리적 에이전트는 옳은 일(바람직함)을 하는 에이전트이다.

결과 주의란 에이전트의 옳은 일은 성과 측도로 구체화한다는 것이다. 성과 측정은 에이전트의 상태가 아니라 환경의 상태가 대상이 된다. 예를 들어, 에이전트가 먼지를 빨아들인 양과 시간마다 깨끗한 방 중에 깨끗한 방에 초점을 맞추는 것이다.

에이전트가 어떻게 행동해야 하는지 기준으로 삼기보다 **환경이 실제로 어떻게 변하는 것이 바람직한지** 기준으로 삼는 것을 성과 측도 설계라고 한다.

### 2-2. 합리성

합리성을 결정하는 요인에는 성과 측도, 사전 지식, 동작들, 지각열이 있다.

이러한 요인을 바탕으로 합리적 에이전트를 정의하면 다음과 같다. :  
_각각의 가능한 지각열에 대해, 합리적인 에이전트는 자신의 지각열과 에이전트의 내장 지식이 제공하는 증거에 기초해서 성과 측정치를 극대화할 만한 동작을 선택해야 한다._

#### 진공청소기 예시

합리적인 예이전트란

- 성과 측정 방식이 어떤 것인지
- 환경에 관해 알려진 것이 무엇인지
- 에이전트에 어떤 감지기와 작동기가 있는지

다음과 같이 가정하기로 하자

- 각 시간 단계에서 깨끗한 사각형 마다 1점 획득
- 진공청소기의 수명의 1,000개의 시간 단계
- 꺠끗한 사각형은 깨끗한 상태를 유지하고 진공청소기가 먼지를 빨아들인 사각형은 깨끗해진다
- Left, Right 동작은 각각 에이전트를 왼쪽, 오른쪽으로 이동시킨다. 단 사각형 밖으로 이동을 불가능하다
- 가능한 동작은 Left, Right, Suck 뿐이다
- 에이전트는 자신의 위치 및 먼지 유무를 정확히 인식한다

### 2-3. 전지, 학습, 자율성

합리성과 전지를 신중하게 구분할 필요가 있다. 합리성은 **기대 성과**를 극대화하는 반면, 완벽함(전지)은 실제 성과를 극대화한다.  
전지한 에이전트는 자신의 동작의 실제 결과를 미리 알고 그에 따라 행동할 수 있다. 그러나 현실에서 그러한 전지전능함은 불가능하다.  
최상의 동작을 하는 에이전트 설계는 불가능하다. 합리적 에이전트의 정의에 의하면 합리적 선택은 오직 지금까지의 지각열에만 의존하기 때문이다.

합리적 에이전트는 정보를 수집(탐험)해야 할 뿐만 아니라 자신이 지각한 것에서 최대한 많은 것을 배워야(학습) 한다. 환경 사전 지식(초기구성)을 가지고 정보를 수집하고 학습을 하는 순서에 따라 자율성을 확보해 나가야 한다.

## 3. 환경의 본성

합리적인 에이전트를 실제로 구축하기 전 과제 환경을 생각한다. 과제 환경은 문제이고, 합리적 에이전트는 해답이다.

### 3-1. 과제 환경의 명시

과제 환경이란 성과 측정 방식과 환경, 그리고 에이전트의 작동기 및 감+지기를 말하고, PEAS(Performance, Environment, Actuators, Sensors)라고도 불린다.

에이전트를 설계할 때 그 과제 환경을 최대한 완전하게 서술하는 것을 항상 첫 단계에 해야한다.

### 3-2. 과제 환경의 속성

- 완전 관찰 가능한 환경 vs 부분 관찰 가능한 환경
  - 동작과 관련된 모든 정보를 관측이 성과 측도를 좌우한다.
- 단일 에이전트 환경(경쟁적) vs 다중 에이전트 환경 (협동적)
- 결정론적 환경 vs 확률론적 환경
  - 다음 상태가 현재 상태와 에이전트 수행 동작으로 결정되는가?
- 일화적 환경 vs 순차적 환경

- 정적 환경 vs 동적 환경
  - 다음 행동을 하는 동안 환경이 변하는가?
- 이산적 환경 vs 연속적 환경
  - 환경의 상태, 시간의 처리방식, 에이전트의 지각 및 동작과 관련
- 기지(known) 환경 vs 미지(unknown) 환경

## 4. 에이전트의 구조

아키텍처와 프로그램을 합쳐 에이전트라고 한다. 에이전트 프로그램이란 에이전트 함수(지각을 동작으로 사상하는 함수)의 구현이다.

에이전트 아키텍처는 물리적 감지기와 작동기를 갖춘 계산 장치로, 지각들을 프로그램에 제공하고, 프로그램을 실행하고, 프로그램이 선택한 동작들을 작동기에 공급해서 동작한다.

### 4-1. 에이전트 프로그램

`TABLE-DRIVEN-AGENT` 프로그램은 각각의 새 지각마다 호출되고 매번 하나의 동작을 돌려준다. 이 프로그램은 완전한 지각열을 메모리 안에 유지한다.

지속 변수는 `percepts` 와 `table` 로 구성되어 있다.  
percepts는 지각들의 순차열로 초기에는 비어있다.  
table은 동작들의 표로 지각열을 색인으로 하고 처음부터 완전히 채워져 있다.

```
function TABLE-DRIVEN-AGENT([location, status]) return 하나의 동작

percept를 percepts의 끝에 추가한다.
action <- LOOKUP(percents, table)
return action;
```

표의 항목을 구성하는데 공간적, 시간적 제약이 있어 표 위주로 에이전트를 구축하는 접근방식은 실패할 수 밖에 없다.

따라서 작은 프로그램으로 가능한 합리적으로 행동을 산출할 수 있는 프로그램을 작성하는 방법을 찾는 것이 핵심적인 도전 과제이다.

### 4-2. 에이전트 종류

에이전트 종류에는 단순 반사 에이전트, 모형 기반 에이전트, 목표 기반 에이저트, 효용 기반 에이전트가 있다.

#### 단순 반사 에이전트

단순 반사 에이전트는 가장 단순한 형태의 에이전트이다. 항상 현재 지각에 근거해서 동작을 선택할 뿐, 지각 역사(percepts)의 나머지 부분은 무시한다.

```
function REFLEX-VACUUM-AGENT([location, status]) return 하나의 동작

if status = Dirty then return Suck
else if location = A then return Right
else if location B then return Left
```

if-then 규칙으로 동작하며, 만약(if) 앞 차가 제동 중이라면(then) 제동 시작한다.

![](https://velog.velcdn.com/images/yeonsubaek/post/4d9467bc-a1da-4444-98c5-4c691ed54e03/image.jpeg)

```
function SIMPLE-REFLEX-AGENT(percept) returns 하나의 동작
  지속 변수: rules, 조건-동작 규칙들의 집합.

state <- INTERPRET-INPUT(percept) // 지각이 들어왔을 때 지각에 대한 상태 해석
rule <- PULE-MATCH(state, rules)
action <- rule.ACTION
return action
```

단순 반사 에이전트는 단순하고 구현이 쉽다는 장점이 있지만, 제한적이고 환경이 완전 관찰일 때만 사용할 수 있고 관측 블가능한 요인이 있을 때 문제가 발생한다는 단점이 있다.

#### 모형 기반 반사 에이전트

모형 기반 반사 에이전트는 관측한 환경 정보를 에이전트가 계속 유지하여 부분 관찰 가능성을 처리하는 가장 효과적인 방법이다. 즉, 현재 상태의 관찰되지 않은 일부 측면을 반영하는 내부 상태를 유지하는 것이다.

이 에이전트는 감지기 모형(sensor model)과 전이 모형(transition model)으로 내부 상태를 갱신한다. 감지기 모형은 세계가 시간 흐름에 따라 진화하는 것이고, 전이 모형은 세계의 상태가 에이전트의 지각에 반영되는 것이다.

![](https://velog.velcdn.com/images/yeonsubaek/post/9d726531-eb94-4347-85c9-186f1c7d3097/image.jpeg)

```
function MODEL-BASED-REFLEX-AGENT(percept) returns 하나의 동작
  지속 변수: state, 에이전트가 현재 인식하고 있는 상태
  		   model, 다음 상태가 현재 상태와 동작에 어떻게 의존하는지에 대한 서술
           rules, 조건-동작 규칙들의 집합
           action, 가장 최근 동작(초기에는 없음)

state <- UPDATE-STATE(state, action, percept, model)
rule <- RULE-MATCH(state, rules)
action <- rule.ACTION
return action
```

#### 목표 기반 에이전트

목표 기반 에이전트는 세계의 상태를 추적한 뿐만 아니라 자신이 추구하는 목표들의 집합도 추적하며, 그 목표들의 달성으로 이어지는 동작을 선택한다.

![](https://velog.velcdn.com/images/yeonsubaek/post/0b678f84-e9e7-41be-9d29-4048206b4db8/image.jpeg)

#### 효용 기반 에이전트

효용 기반 에이전트는 목표를 달성했는지 여부가 중요하다. 목표를 달성하는 방법은 다수가 존재하나 가치는 다르다. 예를 들어 택시가 목적지에 도달하는 동작열은 많이 있지만, 소비 시간이나 안전성, 신뢰성, 비용은 모두 다르다.

효용을 기준으로 성과를 측정하는 것은 목표 기반보다 일반적인 방식이다. 유용함의 정도를 측정하기 위해 효용 함수가 필요하다.

목표가 그리 적합하지 않은 상황에서 효용 기반 에이전트가 합리적 결정을 내릴 수 있는 두 가지 경우가 존재한다. 첫 번째, 목표들이 서로 충돌해서 오직 일부 목표만 달성이 가능한 경우 효용 함수가 적절한 절충선을 지정해준다. 두 번째, 에이전트가 추구할 만한 목표가 여러 개이지만 그 중 확실하게 달성할 수 있는 것이 하나도 없을 때 효용 함수는 목표의 중요도에 비한 성공 가능성을 추정하는 방법을 제공한다.

![](https://velog.velcdn.com/images/yeonsubaek/post/2f48bdfc-9dbd-4d05-b533-a56515a3f486/image.jpeg)

### 4-3. 학습하는 에이전트

학습 에이전트는 네 가지 개념적 구성요소를 가지고 있다.

- 학습의 진척을 책임지는 **학습 요소**
- 외부 동작의 선택을 책임지는 **수행 요소**
- 에이전트가 얼마나 잘 하고 있는지에 관한 의견을 제공하는 **비평가**
- 새롭고 배울 점이 있는 경함으로 이어질 동작들을 제시하는 문제 **생성기** 0

에이전트를 설계할 떄 첫 번째 질문은 "이 능력을 어떻게 배우게 할 것인가?"가 아니라 **"이 능력을 배웠을 때 에이전트가 그것을 실제로 수행하려면 어떤 종류의 수행 요소가 필요한가?"**이다.

![](https://velog.velcdn.com/images/yeonsubaek/post/a27158f5-0ded-4da1-84ff-720b7cef6aa8/image.jpeg)

### 4-4. 에이전트 프로그램 구성요소들의 작동 방식

상태들과 그들 사이의 전이를 표현하는 세 가지 방법이 있다.

- 원자적 표현: 하나의 상태는 내부 구조가 없는 블랙박스이다.
- 분해된 표현: 하나의 상태는 특성값들의 벡터로 이루어진다. 특성값들은 부울 값, 실수 값, 고정된 기호 집합의 한 기호 등 어떤 것일 수도 있다.
- 구조적 표현: 하나의 상태는 객체들을 포함할 수 있으며 각 객체는 자신의 특성들을 가질 수 있다. 또한 상태에는 그러한 객체들 사이의 관계들도 포함된다.

## 5. 요약

- 에이전트는 자신의 환경을 지각하고 그 환경 안에서 행동하는 어떤 것이다. 에이전트의 에이전트 함수는 임의의 지각열에 대한 반응으로 에이전트가 취할 동작을 지정한다.
- 성과 측정은 환경 안에서의 에이전트의 행동을 평가한다. 합리적 에이전트는 지금까지의 지각열이 주어졌을 때 성과 측정의 기대 값을 최대화하는 방식으로 행동한다.
- 과제 환경 명세에는 성과 측정 방식, 외부 환경, 작동기, 감지기들이 포함된다. 에이전트 설계의 첫 단계는 항상 과제 환경을 최대한 자세하게 명시하는 것이어야 한다.
- 과제 환경은 여러 주요 차원들로 분류된다. 과제 환경은 완전 관찰 가능이거나 부분 관찰 가능이고, 단일 에이전트이거나 다중 에이전트이고, 결정론적이나 확률론적이고, 일화적이거나 순차적이고, 정적이거나 동적이고, 이산적이거나 연속적이고, 알려져 있거나 미지이다.
- 에이전트 프로그램은 에이전트 함수를 구현한다. 명시적으로 주어진, 그리고 의사 결정 과정에서 쓰이는 정보의 종류를 반영하는 다양한 기본 에이전트 프로그램 설계들이 존재한다. 그 설계들은 그 효율성과 간결성, 유연성이 각자 다르다. 어떤 설계가 적절한지는 환경의 성격에 따라 달라진다.
- 단순 반사 세이전트는 지각에 직접 반응하는 반면 모형 기반 반사 에이전트는 현재 지각으로는 명확히 알기 어려운 측면들을 추적하는 내부 상태를 유지한다. 목표 기반 에이전트는 자신의 목표들을 달성하기 위해 행동하며, 효용 기반 에이전트는 자신의 기대 효용 또는 기대 행복도를 최대화하려 한다.
- 모든 에이전트는 학습을 통해서 자신의 성과를 개선할 수 있다.

---

학교 인공지능 강의를 듣고 정리한 내용
